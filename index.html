<!DOCTYPE HTML>
<html>
	<head>
		<title>ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data</title>
		<meta charset="utf-8" />
		 <meta name="viewport" content="width=1000">
		<link rel="stylesheet" href="assets/css/main.css" />

		<!-- Google tag (gtag.js) -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-LS6L6LB7RX"></script>
		<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'G-LS6L6LB7RX');
		</script>

		<meta property="og:url"           content="https://umi-gripper.github.io" />
	    <meta property="og:type"          content="website" />
	    <meta property="og:title"         content="ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data" />
	    <meta property="og:description"   content="" />
	    

	</head>
	<body id="top">


		<!-- Main -->
			<div id="main" style="padding-bottom:1em; padding-top: 5em; width: 60em; max-width: 70em; margin-left: auto; margin-right: auto;">

                <h1 style="text-align: center; margin-bottom: 0; color: #e0218a; font-size: 300%">ManiWAV:</h1>
                <h2 style="text-align: center; white-space: nowrap; font-size: 200%">Learning Robot <span style="color:#e0218a">Mani</span>pulation 
                    from In-the-<span style="color:#e0218a">W</span>ild <span style="color:#e0218a">A</span>udio-<span style="color:#e0218a">V</span>isual Data</h2>

                <div class="box alt" style="margin-bottom: 1em;">
                    <div class="row 50% uniform" style="width: 100%; color: black;">
                        <div class="2u" style="font-size: 1em; line-height: 1.5em; text-align: center; width: 50%">
                            Human Demonstration with Audio-Visual Feedback
                        </div>
                        
                        <div class="2u" style="font-size: 1em; line-height: 1.5em; text-align: center; width: 50%">
                            Robot Policy Rollout
                        </div>
                    </div>
                    <div class="row 50% uniform" style="width: 100%;">
                        <div class="2u" style="font-size: 1em; line-height: 1.5em; text-align: center; width: 50%">
                            <span class="image fit">
                                <video controls autoplay muted loop style="width: 100%; margin-right: 5%;">
                                    <source src="videos/demo.mp4" type="video/mp4">	
                                </video>
                            </span>
                            <!-- Dish Washing Data Collection-->
                        </div>

                        <div class="2u" style="font-size: 1em; line-height: 1.5em; text-align: center; width: 50%">
                            <span class="image fit">
                                <video controls autoplay muted loop style="width: 100%; margin-top:5%; margin-right: 5%;">
                                    <source src="videos/white-pan-itw.mp4" type="video/mp4">	
                                </video>
                                <!-- <div style="position: absolute; top: 2%; right: 1%">
                                    <p style="color: white; text-align: center; font-size: 2em">1x</p>
                                </div> -->
                            </span>
                            <!-- Dish Washing -->
                        </div>
                    </div>
                </div>

                <p style="color: black;">Audio signals provide rich information for the robot interaction and object properties through contact. These information can surprisingly ease the learning of contact-rich robot manipulation skills, especially when the visual information alone is ambiguous or incomplete. However, the usage of audio data in robot manipulation has been constrained to teleoperated demonstrations collected by either attaching a microphone to the robot or object, which significantly limits its usage in robot learning pipelines. In this work, we introduce ManiWAV: an 'ear-in-hand' data collection device to collect in-the-wild human demonstrations with synchronous audio and visual feedback, and a corresponding policy interface to learn robot manipulation policy directly from the demonstrations. We demonstrate the capabilities of our system through four contact-rich manipulation tasks that require either passively sensing the contact events and modes, or actively sensing the object surface materials and states. In addition, we show that our system can generalize to unseen in-the-wild environments, by learning from diverse in-the-wild human demonstrations.
                </p>
					
			</div>

			<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.poptrox.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
	</body>
</html>